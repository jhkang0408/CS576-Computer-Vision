{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS576Assignment #1: BoVW classification의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfG_todxMT6c"
      },
      "source": [
        "CS576 Assignment #1: Image Classification using Bag of Visual Words (BoVW) \n",
        "====\n",
        "Primary TA : Jaehoon Yoo\n",
        "\n",
        "TA's E-mail : wogns98@kaist.ac.kr, whieya@kaist.ac.kr\n",
        "\n",
        "QnA Channel: https://join.slack.com/t/kaistcs576/shared_invite/zt-o3gqak0y-yj3NCb_SQFxVkqO0U6PWYw\n",
        "## Instruction\n",
        "- In this assignment, we will classify the images into five categories (aeroplane, backgrounds, car, horse, motorcycle, person) using Bag of Visual Word (BoVW) and Support Vector Machine (SVM).\n",
        " \n",
        "- We will extract the SIFT descriptors from the images and construct a codebook. After that, we will encode the images to histogram features using codebook, and train the classifier using those features.\n",
        "\n",
        "- As you follow the given steps, fill in the section marked ***Problem*** with the appropriate code. There are **7 problems** in total.\n",
        "    - For **Problem 1 ~ Problem 4**, you will get full credits (10pt each) if you implement correctly.  \n",
        "    - For **Problem 5 ~ Problem 7**, you **have to write a discussion about the results** as well as implementing the codes. Each problem takes 5pt for the correct implementation and 5 pt for proper discussion. In other words, you will get only 5pt without proper discussion even if you correctly implement the codes. To get full credit for discussion, please follow **Discussion Guidelines**.\n",
        "\n",
        "## Discussion Guidelines\n",
        "- You should write a discussion about **Problem 5 ~ Problem 7** on the **Discussion and Analysis** section. \n",
        "- Simply reporting the scores (e.g. classification accuracy) is not considered as a discussion.\n",
        "- For each problem's discussion, you should explain and compare how each method improves the results. \n",
        "\n",
        "## Submission guidelines\n",
        "- Your code and report will be all in Colab. Copy this example to your google drive and edit it to complete your assignment. \n",
        "- <font color=\"red\"> You will get the full credit **only if** you complete the code **and** write a discussion of the results in the discussion section at the bottom of this page. </font>\n",
        "- We should be able to reproduce your results using your code. Please double-check if your code runs without error and reproduces your results. Submissions failed to run or reproduce the results will get a substantial penalty. \n",
        "- <font color=\"red\"> **DO NOT modify any of the skeleton codes when you submit.** Please write your codes only in the designated area. </font>\n",
        "- As a proof that you've ran this code by yourself, **make sure your notebook contains the output of each code block.**\n",
        "\n",
        "## Deliverables\n",
        "- Download your Colab notebook, and submit it in a format: [StudentID].ipynb.\n",
        "- Your assignment should be submitted through KLMS. All other submissions (e.g., via email) will not be considered as valid submissions. \n",
        "\n",
        "## Due date\n",
        "- **23:59:59 April 7th.**\n",
        "- Late submission is allowed until 23:59:59 April 9th.\n",
        "- Late submission will be applied 20% penalty.\n",
        "\n",
        "\n",
        "\n",
        "## Questions\n",
        "- Please use the SLACK channel (https://join.slack.com/t/kaistcs576/shared_invite/zt-o3gqak0y-yj3NCb_SQFxVkqO0U6PWYw) as a main communication channel. \n",
        "When you post questions, please make it public so that all students can share the information. Please use the prefix \"[Assignment 1]\" in the subject for all questions regarding this assignment (e.g., [Assignment 1] Regarding the grading policy).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RysBzJQFeIkg"
      },
      "source": [
        "## Step 0: Set the enviroments\n",
        "For this assignment, you need the special library for extracting features & training classifier (cyvlfeat & sklearn).\n",
        "This step takes about 5~15 minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26drrtRufRbK"
      },
      "source": [
        "###  0-1: Download cyvlfeat library & conda"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEjDierhsAZ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a76c9e3-9f6c-4312-c8a3-64b38ed9a3aa"
      },
      "source": [
        "# install conda on colab\n",
        "!wget -c https://repo.continuum.io/archive/Anaconda3-5.3.1-Linux-x86_64.sh\n",
        "!chmod +x Anaconda3-5.3.1-Linux-x86_64.sh\n",
        "!bash ./Anaconda3-5.3.1-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "# install cyvlfeat\n",
        "# Reference : https://anaconda.org/menpo/cyvlfeat\n",
        "# Update URL (2021/03/22)\n",
        "!conda install -c menpo cyvlfeat python==3.7 -y\n",
        "!conda install cython numpy scipy -y\n",
        "\n",
        "import sys\n",
        "sys.path.append('/cyvlfeat')\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "\n",
        "!git clone https://github.com/menpo/cyvlfeat.git /cyvlfeat\n",
        "!cd /cyvlfeat && CFLAGS=\"-I$CONDA_PREFIX/include\" LDFLAGS=\"-L$CONDA_PREFIX/lib\" pip install -e ./"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-06 12:28:17--  https://repo.continuum.io/archive/Anaconda3-5.3.1-Linux-x86_64.sh\n",
            "Resolving repo.continuum.io (repo.continuum.io)... 104.18.201.79, 104.18.200.79, 2606:4700::6812:c84f, ...\n",
            "Connecting to repo.continuum.io (repo.continuum.io)|104.18.201.79|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://repo.anaconda.com/archive/Anaconda3-5.3.1-Linux-x86_64.sh [following]\n",
            "--2021-04-06 12:28:17--  https://repo.anaconda.com/archive/Anaconda3-5.3.1-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.130.3, 104.16.131.3, 2606:4700::6810:8303, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.130.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 667976437 (637M) [application/x-sh]\n",
            "Saving to: ‘Anaconda3-5.3.1-Linux-x86_64.sh’\n",
            "\n",
            "Anaconda3-5.3.1-Lin 100%[===================>] 637.03M   100MB/s    in 5.8s    \n",
            "\n",
            "2021-04-06 12:28:23 (109 MB/s) - ‘Anaconda3-5.3.1-Linux-x86_64.sh’ saved [667976437/667976437]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "reinstalling: python-3.7.0-hc3d631a_0 ...\n",
            "Python 3.7.0\n",
            "reinstalling: blas-1.0-mkl ...\n",
            "reinstalling: ca-certificates-2018.03.07-0 ...\n",
            "reinstalling: conda-env-2.6.0-1 ...\n",
            "reinstalling: intel-openmp-2019.0-118 ...\n",
            "reinstalling: libgcc-ng-8.2.0-hdf63c60_1 ...\n",
            "reinstalling: libgfortran-ng-7.3.0-hdf63c60_0 ...\n",
            "reinstalling: libstdcxx-ng-8.2.0-hdf63c60_1 ...\n",
            "reinstalling: bzip2-1.0.6-h14c3975_5 ...\n",
            "reinstalling: expat-2.2.6-he6710b0_0 ...\n",
            "reinstalling: fribidi-1.0.5-h7b6447c_0 ...\n",
            "reinstalling: gmp-6.1.2-h6c8ec71_1 ...\n",
            "reinstalling: graphite2-1.3.12-h23475e2_2 ...\n",
            "reinstalling: icu-58.2-h9c2bf20_1 ...\n",
            "reinstalling: jbig-2.1-hdba287a_0 ...\n",
            "reinstalling: jpeg-9b-h024ee3a_2 ...\n",
            "reinstalling: libffi-3.2.1-hd88cf55_4 ...\n",
            "reinstalling: libsodium-1.0.16-h1bed415_0 ...\n",
            "reinstalling: libtool-2.4.6-h544aabb_3 ...\n",
            "reinstalling: libuuid-1.0.3-h1bed415_2 ...\n",
            "reinstalling: libxcb-1.13-h1bed415_1 ...\n",
            "reinstalling: lzo-2.10-h49e0be7_2 ...\n",
            "reinstalling: mkl-2019.0-118 ...\n",
            "reinstalling: ncurses-6.1-hf484d3e_0 ...\n",
            "reinstalling: openssl-1.0.2p-h14c3975_0 ...\n",
            "reinstalling: patchelf-0.9-hf484d3e_2 ...\n",
            "reinstalling: pcre-8.42-h439df22_0 ...\n",
            "reinstalling: pixman-0.34.0-hceecf20_3 ...\n",
            "reinstalling: snappy-1.1.7-hbae5bb6_3 ...\n",
            "reinstalling: xz-5.2.4-h14c3975_4 ...\n",
            "reinstalling: yaml-0.1.7-had09818_2 ...\n",
            "reinstalling: zlib-1.2.11-ha838bed_2 ...\n",
            "reinstalling: blosc-1.14.4-hdbcaa40_0 ...\n",
            "reinstalling: glib-2.56.2-hd408876_0 ...\n",
            "reinstalling: hdf5-1.10.2-hba1933b_1 ...\n",
            "reinstalling: libedit-3.1.20170329-h6b74fdf_2 ...\n",
            "reinstalling: libpng-1.6.34-hb9fc6fc_0 ...\n",
            "reinstalling: libssh2-1.8.0-h9cfc8f7_4 ...\n",
            "reinstalling: libtiff-4.0.9-he85c1e1_2 ...\n",
            "reinstalling: libxml2-2.9.8-h26e45fe_1 ...\n",
            "reinstalling: mpfr-4.0.1-hdf1c602_3 ...\n",
            "reinstalling: pandoc-1.19.2.1-hea2e7c5_1 ...\n",
            "reinstalling: readline-7.0-h7b6447c_5 ...\n",
            "reinstalling: tk-8.6.8-hbc83047_0 ...\n",
            "reinstalling: zeromq-4.2.5-hf484d3e_1 ...\n",
            "reinstalling: dbus-1.13.2-h714fa37_1 ...\n",
            "reinstalling: freetype-2.9.1-h8a8886c_1 ...\n",
            "reinstalling: gstreamer-1.14.0-hb453b48_1 ...\n",
            "reinstalling: libcurl-7.61.0-h1ad7b7a_0 ...\n",
            "reinstalling: libxslt-1.1.32-h1312cb7_0 ...\n",
            "reinstalling: mpc-1.1.0-h10f8cd9_1 ...\n",
            "reinstalling: sqlite-3.24.0-h84994c4_0 ...\n",
            "reinstalling: unixodbc-2.3.7-h14c3975_0 ...\n",
            "reinstalling: curl-7.61.0-h84994c4_0 ...\n",
            "reinstalling: fontconfig-2.13.0-h9420a91_0 ...\n",
            "reinstalling: gst-plugins-base-1.14.0-hbbd80ab_1 ...\n",
            "reinstalling: alabaster-0.7.11-py37_0 ...\n",
            "reinstalling: appdirs-1.4.3-py37h28b3542_0 ...\n",
            "reinstalling: asn1crypto-0.24.0-py37_0 ...\n",
            "reinstalling: atomicwrites-1.2.1-py37_0 ...\n",
            "reinstalling: attrs-18.2.0-py37h28b3542_0 ...\n",
            "reinstalling: backcall-0.1.0-py37_0 ...\n",
            "reinstalling: backports-1.0-py37_1 ...\n",
            "reinstalling: beautifulsoup4-4.6.3-py37_0 ...\n",
            "reinstalling: bitarray-0.8.3-py37h14c3975_0 ...\n",
            "reinstalling: boto-2.49.0-py37_0 ...\n",
            "reinstalling: cairo-1.14.12-h8948797_3 ...\n",
            "reinstalling: certifi-2018.8.24-py37_1 ...\n",
            "reinstalling: chardet-3.0.4-py37_1 ...\n",
            "reinstalling: click-6.7-py37_0 ...\n",
            "reinstalling: cloudpickle-0.5.5-py37_0 ...\n",
            "reinstalling: colorama-0.3.9-py37_0 ...\n",
            "reinstalling: constantly-15.1.0-py37h28b3542_0 ...\n",
            "reinstalling: contextlib2-0.5.5-py37_0 ...\n",
            "reinstalling: dask-core-0.19.1-py37_0 ...\n",
            "reinstalling: decorator-4.3.0-py37_0 ...\n",
            "reinstalling: defusedxml-0.5.0-py37_1 ...\n",
            "reinstalling: docutils-0.14-py37_0 ...\n",
            "reinstalling: entrypoints-0.2.3-py37_2 ...\n",
            "reinstalling: et_xmlfile-1.0.1-py37_0 ...\n",
            "reinstalling: fastcache-1.0.2-py37h14c3975_2 ...\n",
            "reinstalling: filelock-3.0.8-py37_0 ...\n",
            "reinstalling: glob2-0.6-py37_0 ...\n",
            "reinstalling: gmpy2-2.0.8-py37h10f8cd9_2 ...\n",
            "reinstalling: greenlet-0.4.15-py37h7b6447c_0 ...\n",
            "reinstalling: heapdict-1.0.0-py37_2 ...\n",
            "reinstalling: idna-2.7-py37_0 ...\n",
            "reinstalling: imagesize-1.1.0-py37_0 ...\n",
            "reinstalling: incremental-17.5.0-py37_0 ...\n",
            "reinstalling: ipython_genutils-0.2.0-py37_0 ...\n",
            "reinstalling: itsdangerous-0.24-py37_1 ...\n",
            "reinstalling: jdcal-1.4-py37_0 ...\n",
            "reinstalling: jeepney-0.3.1-py37_0 ...\n",
            "reinstalling: kiwisolver-1.0.1-py37hf484d3e_0 ...\n",
            "reinstalling: lazy-object-proxy-1.3.1-py37h14c3975_2 ...\n",
            "reinstalling: llvmlite-0.24.0-py37hdbcaa40_0 ...\n",
            "reinstalling: locket-0.2.0-py37_1 ...\n",
            "reinstalling: lxml-4.2.5-py37hefd8a0e_0 ...\n",
            "reinstalling: markupsafe-1.0-py37h14c3975_1 ...\n",
            "reinstalling: mccabe-0.6.1-py37_1 ...\n",
            "reinstalling: mistune-0.8.3-py37h14c3975_1 ...\n",
            "reinstalling: mkl-service-1.1.2-py37h90e4bf4_5 ...\n",
            "reinstalling: mpmath-1.0.0-py37_2 ...\n",
            "reinstalling: msgpack-python-0.5.6-py37h6bb024c_1 ...\n",
            "reinstalling: numpy-base-1.15.1-py37h81de0dd_0 ...\n",
            "reinstalling: olefile-0.46-py37_0 ...\n",
            "reinstalling: pandocfilters-1.4.2-py37_1 ...\n",
            "reinstalling: parso-0.3.1-py37_0 ...\n",
            "reinstalling: path.py-11.1.0-py37_0 ...\n",
            "reinstalling: pep8-1.7.1-py37_0 ...\n",
            "reinstalling: pickleshare-0.7.4-py37_0 ...\n",
            "reinstalling: pkginfo-1.4.2-py37_1 ...\n",
            "reinstalling: pluggy-0.7.1-py37h28b3542_0 ...\n",
            "reinstalling: ply-3.11-py37_0 ...\n",
            "reinstalling: psutil-5.4.7-py37h14c3975_0 ...\n",
            "reinstalling: ptyprocess-0.6.0-py37_0 ...\n",
            "reinstalling: py-1.6.0-py37_0 ...\n",
            "reinstalling: pyasn1-0.4.4-py37h28b3542_0 ...\n",
            "reinstalling: pycodestyle-2.4.0-py37_0 ...\n",
            "reinstalling: pycosat-0.6.3-py37h14c3975_0 ...\n",
            "reinstalling: pycparser-2.18-py37_1 ...\n",
            "reinstalling: pycrypto-2.6.1-py37h14c3975_9 ...\n",
            "reinstalling: pycurl-7.43.0.2-py37hb7f436b_0 ...\n",
            "reinstalling: pyflakes-2.0.0-py37_0 ...\n",
            "reinstalling: pyodbc-4.0.24-py37he6710b0_0 ...\n",
            "reinstalling: pyparsing-2.2.0-py37_1 ...\n",
            "reinstalling: pysocks-1.6.8-py37_0 ...\n",
            "reinstalling: pytz-2018.5-py37_0 ...\n",
            "reinstalling: pyyaml-3.13-py37h14c3975_0 ...\n",
            "reinstalling: pyzmq-17.1.2-py37h14c3975_0 ...\n",
            "reinstalling: qt-5.9.6-h8703b6f_2 ...\n",
            "reinstalling: qtpy-1.5.0-py37_0 ...\n",
            "reinstalling: rope-0.11.0-py37_0 ...\n",
            "reinstalling: ruamel_yaml-0.15.46-py37h14c3975_0 ...\n",
            "reinstalling: send2trash-1.5.0-py37_0 ...\n",
            "reinstalling: simplegeneric-0.8.1-py37_2 ...\n",
            "reinstalling: sip-4.19.8-py37hf484d3e_0 ...\n",
            "reinstalling: six-1.11.0-py37_1 ...\n",
            "reinstalling: snowballstemmer-1.2.1-py37_0 ...\n",
            "reinstalling: sortedcontainers-2.0.5-py37_0 ...\n",
            "reinstalling: sphinxcontrib-1.0-py37_1 ...\n",
            "reinstalling: sqlalchemy-1.2.11-py37h7b6447c_0 ...\n",
            "reinstalling: tblib-1.3.2-py37_0 ...\n",
            "reinstalling: testpath-0.3.1-py37_0 ...\n",
            "reinstalling: toolz-0.9.0-py37_0 ...\n",
            "reinstalling: tornado-5.1-py37h14c3975_0 ...\n",
            "reinstalling: tqdm-4.26.0-py37h28b3542_0 ...\n",
            "reinstalling: unicodecsv-0.14.1-py37_0 ...\n",
            "reinstalling: wcwidth-0.1.7-py37_0 ...\n",
            "reinstalling: webencodings-0.5.1-py37_1 ...\n",
            "reinstalling: werkzeug-0.14.1-py37_0 ...\n",
            "reinstalling: wrapt-1.10.11-py37h14c3975_2 ...\n",
            "reinstalling: xlrd-1.1.0-py37_1 ...\n",
            "reinstalling: xlsxwriter-1.1.0-py37_0 ...\n",
            "reinstalling: xlwt-1.3.0-py37_0 ...\n",
            "reinstalling: zope-1.0-py37_1 ...\n",
            "reinstalling: astroid-2.0.4-py37_0 ...\n",
            "reinstalling: automat-0.7.0-py37_0 ...\n",
            "reinstalling: babel-2.6.0-py37_0 ...\n",
            "reinstalling: backports.shutil_get_terminal_size-1.0.0-py37_2 ...\n",
            "reinstalling: cffi-1.11.5-py37he75722e_1 ...\n",
            "reinstalling: cycler-0.10.0-py37_0 ...\n",
            "reinstalling: cytoolz-0.9.0.1-py37h14c3975_1 ...\n",
            "reinstalling: harfbuzz-1.8.8-hffaf4a1_0 ...\n",
            "reinstalling: html5lib-1.0.1-py37_0 ...\n",
            "reinstalling: hyperlink-18.0.0-py37_0 ...\n",
            "reinstalling: jedi-0.12.1-py37_0 ...\n",
            "reinstalling: more-itertools-4.3.0-py37_0 ...\n",
            "reinstalling: multipledispatch-0.6.0-py37_0 ...\n",
            "reinstalling: networkx-2.1-py37_0 ...\n",
            "reinstalling: nltk-3.3.0-py37_0 ...\n",
            "reinstalling: openpyxl-2.5.6-py37_0 ...\n",
            "reinstalling: packaging-17.1-py37_0 ...\n",
            "reinstalling: partd-0.3.8-py37_0 ...\n",
            "reinstalling: pathlib2-2.3.2-py37_0 ...\n",
            "reinstalling: pexpect-4.6.0-py37_0 ...\n",
            "reinstalling: pillow-5.2.0-py37heded4f4_0 ...\n",
            "reinstalling: pyasn1-modules-0.2.2-py37_0 ...\n",
            "reinstalling: pyqt-5.9.2-py37h05f1152_2 ...\n",
            "reinstalling: python-dateutil-2.7.3-py37_0 ...\n",
            "reinstalling: qtawesome-0.4.4-py37_0 ...\n",
            "reinstalling: setuptools-40.2.0-py37_0 ...\n",
            "reinstalling: singledispatch-3.4.0.3-py37_0 ...\n",
            "reinstalling: sortedcollections-1.0.1-py37_0 ...\n",
            "reinstalling: sphinxcontrib-websupport-1.1.0-py37_1 ...\n",
            "reinstalling: sympy-1.2-py37_0 ...\n",
            "reinstalling: terminado-0.8.1-py37_1 ...\n",
            "reinstalling: traitlets-4.3.2-py37_0 ...\n",
            "reinstalling: zict-0.1.3-py37_0 ...\n",
            "reinstalling: zope.interface-4.5.0-py37h14c3975_0 ...\n",
            "reinstalling: bleach-2.1.4-py37_0 ...\n",
            "reinstalling: clyent-1.2.2-py37_1 ...\n",
            "reinstalling: cryptography-2.3.1-py37hc365091_0 ...\n",
            "reinstalling: cython-0.28.5-py37hf484d3e_0 ...\n",
            "reinstalling: distributed-1.23.1-py37_0 ...\n",
            "reinstalling: get_terminal_size-1.0.0-haa9412d_0 ...\n",
            "reinstalling: gevent-1.3.6-py37h7b6447c_0 ...\n",
            "reinstalling: isort-4.3.4-py37_0 ...\n",
            "reinstalling: jinja2-2.10-py37_0 ...\n",
            "reinstalling: jsonschema-2.6.0-py37_0 ...\n",
            "reinstalling: jupyter_core-4.4.0-py37_0 ...\n",
            "reinstalling: navigator-updater-0.2.1-py37_0 ...\n",
            "reinstalling: nose-1.3.7-py37_2 ...\n",
            "reinstalling: pango-1.42.4-h049681c_0 ...\n",
            "reinstalling: pygments-2.2.0-py37_0 ...\n",
            "reinstalling: pytest-3.8.0-py37_0 ...\n",
            "reinstalling: wheel-0.31.1-py37_0 ...\n",
            "reinstalling: flask-1.0.2-py37_1 ...\n",
            "reinstalling: jupyter_client-5.2.3-py37_0 ...\n",
            "reinstalling: nbformat-4.4.0-py37_0 ...\n",
            "reinstalling: pip-10.0.1-py37_0 ...\n",
            "reinstalling: prompt_toolkit-1.0.15-py37_0 ...\n",
            "reinstalling: pylint-2.1.1-py37_0 ...\n",
            "reinstalling: pyopenssl-18.0.0-py37_0 ...\n",
            "reinstalling: pytest-openfiles-0.3.0-py37_0 ...\n",
            "reinstalling: pytest-remotedata-0.3.0-py37_0 ...\n",
            "reinstalling: secretstorage-3.1.0-py37_0 ...\n",
            "reinstalling: flask-cors-3.0.6-py37_0 ...\n",
            "reinstalling: ipython-6.5.0-py37_0 ...\n",
            "reinstalling: keyring-13.2.1-py37_0 ...\n",
            "reinstalling: nbconvert-5.4.0-py37_1 ...\n",
            "reinstalling: service_identity-17.0.0-py37h28b3542_0 ...\n",
            "reinstalling: urllib3-1.23-py37_0 ...\n",
            "reinstalling: ipykernel-4.9.0-py37_1 ...\n",
            "reinstalling: requests-2.19.1-py37_0 ...\n",
            "reinstalling: twisted-18.7.0-py37h14c3975_1 ...\n",
            "reinstalling: anaconda-client-1.7.2-py37_0 ...\n",
            "reinstalling: jupyter_console-5.2.0-py37_1 ...\n",
            "reinstalling: prometheus_client-0.3.1-py37h28b3542_0 ...\n",
            "reinstalling: qtconsole-4.4.1-py37_0 ...\n",
            "reinstalling: sphinx-1.7.9-py37_0 ...\n",
            "reinstalling: spyder-kernels-0.2.6-py37_0 ...\n",
            "reinstalling: anaconda-navigator-1.9.2-py37_0 ...\n",
            "reinstalling: anaconda-project-0.8.2-py37_0 ...\n",
            "reinstalling: notebook-5.6.0-py37_0 ...\n",
            "reinstalling: numpydoc-0.8.0-py37_0 ...\n",
            "reinstalling: jupyterlab_launcher-0.13.1-py37_0 ...\n",
            "reinstalling: spyder-3.3.1-py37_1 ...\n",
            "reinstalling: widgetsnbextension-3.4.1-py37_0 ...\n",
            "reinstalling: ipywidgets-7.4.1-py37_0 ...\n",
            "reinstalling: jupyterlab-0.34.9-py37_0 ...\n",
            "reinstalling: _ipyw_jlab_nb_ext_conf-0.1.0-py37_0 ...\n",
            "reinstalling: jupyter-1.0.0-py37_7 ...\n",
            "reinstalling: bokeh-0.13.0-py37_0 ...\n",
            "reinstalling: bottleneck-1.2.1-py37h035aef0_1 ...\n",
            "reinstalling: conda-4.5.11-py37_0 ...\n",
            "reinstalling: conda-build-3.15.1-py37_0 ...\n",
            "reinstalling: datashape-0.5.4-py37_1 ...\n",
            "reinstalling: h5py-2.8.0-py37h989c5e5_3 ...\n",
            "reinstalling: imageio-2.4.1-py37_0 ...\n",
            "reinstalling: matplotlib-2.2.3-py37hb69df0a_0 ...\n",
            "reinstalling: mkl_fft-1.0.4-py37h4414c95_1 ...\n",
            "reinstalling: mkl_random-1.0.1-py37h4414c95_1 ...\n",
            "reinstalling: numpy-1.15.1-py37h1d66e8a_0 ...\n",
            "reinstalling: numba-0.39.0-py37h04863e7_0 ...\n",
            "reinstalling: numexpr-2.6.8-py37hd89afb7_0 ...\n",
            "reinstalling: pandas-0.23.4-py37h04863e7_0 ...\n",
            "reinstalling: pytest-arraydiff-0.2-py37h39e3cac_0 ...\n",
            "reinstalling: pytest-doctestplus-0.1.3-py37_0 ...\n",
            "reinstalling: pywavelets-1.0.0-py37hdd07704_0 ...\n",
            "reinstalling: scipy-1.1.0-py37hfa4b5c9_1 ...\n",
            "reinstalling: bkcharts-0.2-py37_0 ...\n",
            "reinstalling: dask-0.19.1-py37_0 ...\n",
            "reinstalling: patsy-0.5.0-py37_0 ...\n",
            "reinstalling: pytables-3.4.4-py37ha205bf6_0 ...\n",
            "reinstalling: pytest-astropy-0.4.0-py37_0 ...\n",
            "reinstalling: scikit-image-0.14.0-py37hf484d3e_1 ...\n",
            "reinstalling: scikit-learn-0.19.2-py37h4989274_0 ...\n",
            "reinstalling: astropy-3.0.4-py37h14c3975_0 ...\n",
            "reinstalling: odo-0.5.1-py37_0 ...\n",
            "reinstalling: statsmodels-0.9.0-py37h035aef0_0 ...\n",
            "reinstalling: blaze-0.11.3-py37_0 ...\n",
            "reinstalling: seaborn-0.9.0-py37_0 ...\n",
            "reinstalling: anaconda-5.3.1-py37_0 ...\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Anaconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Anaconda3: /usr/local\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "\n",
            "\n",
            "==> WARNING: A newer version of conda exists. <==\n",
            "  current version: 4.5.11\n",
            "  latest version: 4.9.2\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base -c defaults conda\n",
            "\n",
            "\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - cyvlfeat\n",
            "    - python==3.7\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    libffi-3.2.1               |    hf484d3e_1007          52 KB\n",
            "    zlib-1.2.11                |       h7b6447c_3         120 KB\n",
            "    openssl-1.0.2u             |       h7b6447c_0         3.1 MB\n",
            "    cyvlfeat-0.5.1             |   py37h975b26e_0         2.0 MB  menpo\n",
            "    libgcc-ng-9.1.0            |       hdf63c60_0         8.1 MB\n",
            "    _libgcc_mutex-0.1          |             main           3 KB\n",
            "    vlfeat-0.9.20              |                1         199 KB  menpo\n",
            "    ca-certificates-2021.1.19  |       h06a4308_1         125 KB\n",
            "    six-1.15.0                 |     pyhd3eb1b0_0          13 KB\n",
            "    mkl-service-2.3.0          |   py37he8ac12f_0          56 KB\n",
            "    libedit-3.1.20210216       |       h27cfd23_1         190 KB\n",
            "    mkl_fft-1.3.0              |   py37h54f3939_0         185 KB\n",
            "    ncurses-6.2                |       he6710b0_1         1.1 MB\n",
            "    numpy-1.19.2               |   py37h54aff64_0          21 KB\n",
            "    numpy-base-1.19.2          |   py37hfa32c7d_0         5.2 MB\n",
            "    certifi-2020.12.5          |   py37h06a4308_0         143 KB\n",
            "    mkl-2020.2                 |              256       213.9 MB\n",
            "    wheel-0.36.2               |     pyhd3eb1b0_0          31 KB\n",
            "    xz-5.2.5                   |       h7b6447c_0         438 KB\n",
            "    tk-8.6.10                  |       hbc83047_0         3.2 MB\n",
            "    mkl_random-1.1.1           |   py37h0573a6f_0         375 KB\n",
            "    sqlite-3.33.0              |       h62c20be_0         2.0 MB\n",
            "    pip-21.0.1                 |   py37h06a4308_0         2.0 MB\n",
            "    python-3.7.0               |       h6e4f718_3        30.6 MB\n",
            "    setuptools-52.0.0          |   py37h06a4308_0         921 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       274.1 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    _libgcc_mutex:   0.1-main                     \n",
            "    cyvlfeat:        0.5.1-py37h975b26e_0    menpo\n",
            "    vlfeat:          0.9.20-1                menpo\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    ca-certificates: 2018.03.07-0                  --> 2021.1.19-h06a4308_1    \n",
            "    certifi:         2018.8.24-py37_1              --> 2020.12.5-py37h06a4308_0\n",
            "    libedit:         3.1.20170329-h6b74fdf_2       --> 3.1.20210216-h27cfd23_1 \n",
            "    libffi:          3.2.1-hd88cf55_4              --> 3.2.1-hf484d3e_1007     \n",
            "    libgcc-ng:       8.2.0-hdf63c60_1              --> 9.1.0-hdf63c60_0        \n",
            "    mkl:             2019.0-118                    --> 2020.2-256              \n",
            "    mkl-service:     1.1.2-py37h90e4bf4_5          --> 2.3.0-py37he8ac12f_0    \n",
            "    mkl_fft:         1.0.4-py37h4414c95_1          --> 1.3.0-py37h54f3939_0    \n",
            "    mkl_random:      1.0.1-py37h4414c95_1          --> 1.1.1-py37h0573a6f_0    \n",
            "    ncurses:         6.1-hf484d3e_0                --> 6.2-he6710b0_1          \n",
            "    numpy:           1.15.1-py37h1d66e8a_0         --> 1.19.2-py37h54aff64_0   \n",
            "    numpy-base:      1.15.1-py37h81de0dd_0         --> 1.19.2-py37hfa32c7d_0   \n",
            "    openssl:         1.0.2p-h14c3975_0             --> 1.0.2u-h7b6447c_0       \n",
            "    pip:             10.0.1-py37_0                 --> 21.0.1-py37h06a4308_0   \n",
            "    python:          3.7.0-hc3d631a_0              --> 3.7.0-h6e4f718_3        \n",
            "    setuptools:      40.2.0-py37_0                 --> 52.0.0-py37h06a4308_0   \n",
            "    six:             1.11.0-py37_1                 --> 1.15.0-pyhd3eb1b0_0     \n",
            "    sqlite:          3.24.0-h84994c4_0             --> 3.33.0-h62c20be_0       \n",
            "    tk:              8.6.8-hbc83047_0              --> 8.6.10-hbc83047_0       \n",
            "    wheel:           0.31.1-py37_0                 --> 0.36.2-pyhd3eb1b0_0     \n",
            "    xz:              5.2.4-h14c3975_4              --> 5.2.5-h7b6447c_0        \n",
            "    zlib:            1.2.11-ha838bed_2             --> 1.2.11-h7b6447c_3       \n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "libffi-3.2.1         | 52 KB     | : 100% 1.0/1 [00:00<00:00, 18.79it/s]\n",
            "zlib-1.2.11          | 120 KB    | : 100% 1.0/1 [00:00<00:00, 13.92it/s]\n",
            "openssl-1.0.2u       | 3.1 MB    | : 100% 1.0/1 [00:00<00:00,  1.01it/s]               \n",
            "cyvlfeat-0.5.1       | 2.0 MB    | : 100% 1.0/1 [00:01<00:00,  1.28s/it]               \n",
            "libgcc-ng-9.1.0      | 8.1 MB    | : 100% 1.0/1 [00:02<00:00,  2.23s/it]               \n",
            "_libgcc_mutex-0.1    | 3 KB      | : 100% 1.0/1 [00:00<00:00, 25.18it/s]\n",
            "vlfeat-0.9.20        | 199 KB    | : 100% 1.0/1 [00:00<00:00,  1.91it/s]               \n",
            "ca-certificates-2021 | 125 KB    | : 100% 1.0/1 [00:00<00:00, 16.59it/s]\n",
            "six-1.15.0           | 13 KB     | : 100% 1.0/1 [00:00<00:00, 27.41it/s]\n",
            "mkl-service-2.3.0    | 56 KB     | : 100% 1.0/1 [00:00<00:00, 21.43it/s]\n",
            "libedit-3.1.20210216 | 190 KB    | : 100% 1.0/1 [00:00<00:00,  9.19it/s]\n",
            "mkl_fft-1.3.0        | 185 KB    | : 100% 1.0/1 [00:00<00:00,  9.38it/s]\n",
            "ncurses-6.2          | 1.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.08s/it]               \n",
            "numpy-1.19.2         | 21 KB     | : 100% 1.0/1 [00:00<00:00, 18.28it/s]\n",
            "numpy-base-1.19.2    | 5.2 MB    | : 100% 1.0/1 [00:02<00:00,  2.10s/it]               \n",
            "certifi-2020.12.5    | 143 KB    | : 100% 1.0/1 [00:00<00:00, 16.77it/s]\n",
            "mkl-2020.2           | 213.9 MB  | : 100% 1.0/1 [01:15<00:00, 75.69s/it]                \n",
            "wheel-0.36.2         | 31 KB     | : 100% 1.0/1 [00:00<00:00, 19.24it/s]\n",
            "xz-5.2.5             | 438 KB    | : 100% 1.0/1 [00:00<00:00,  4.21it/s]               \n",
            "tk-8.6.10            | 3.2 MB    | : 100% 1.0/1 [00:01<00:00,  1.42s/it]               \n",
            "mkl_random-1.1.1     | 375 KB    | : 100% 1.0/1 [00:00<00:00,  5.51it/s]              \n",
            "sqlite-3.33.0        | 2.0 MB    | : 100% 1.0/1 [00:00<00:00,  1.73it/s]               \n",
            "pip-21.0.1           | 2.0 MB    | : 100% 1.0/1 [00:01<00:00,  1.17s/it]               \n",
            "python-3.7.0         | 30.6 MB   | : 100% 1.0/1 [00:08<00:00,  8.75s/it]               \n",
            "setuptools-52.0.0    | 921 KB    | : 100% 1.0/1 [00:00<00:00,  1.57it/s]               \n",
            "Preparing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Verifying transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs: \n",
            "    - cython\n",
            "    - numpy\n",
            "    - scipy\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    conda-package-handling-1.7.2|   py37h03888b9_0         977 KB\n",
            "    cython-0.29.22             |   py37h2531618_0         2.1 MB\n",
            "    conda-4.9.2                |   py37h06a4308_0         3.1 MB\n",
            "    scipy-1.6.2                |   py37h91f5cce_0        19.9 MB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        26.0 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "    conda-package-handling: 1.7.2-py37h03888b9_0 \n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "    conda:                  4.5.11-py37_0         --> 4.9.2-py37h06a4308_0  \n",
            "    cython:                 0.28.5-py37hf484d3e_0 --> 0.29.22-py37h2531618_0\n",
            "    scipy:                  1.1.0-py37hfa4b5c9_1  --> 1.6.2-py37h91f5cce_0  \n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "conda-package-handli | 977 KB    | : 100% 1.0/1 [00:00<00:00,  2.06it/s]               \n",
            "cython-0.29.22       | 2.1 MB    | : 100% 1.0/1 [00:00<00:00,  1.12it/s]               \n",
            "conda-4.9.2          | 3.1 MB    | : 100% 1.0/1 [00:01<00:00,  1.22s/it]               \n",
            "scipy-1.6.2          | 19.9 MB   | : 100% 1.0/1 [00:06<00:00,  6.14s/it]               \n",
            "Preparing transaction: / \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bdone\n",
            "Executing transaction: \\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Cloning into '/cyvlfeat'...\n",
            "remote: Enumerating objects: 48, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 1097 (delta 9), reused 23 (delta 7), pack-reused 1049\u001b[K\n",
            "Receiving objects: 100% (1097/1097), 1.38 MiB | 1.44 MiB/s, done.\n",
            "Resolving deltas: 100% (602/602), done.\n",
            "Obtaining file:///cyvlfeat\n",
            "Installing collected packages: cyvlfeat\n",
            "  Attempting uninstall: cyvlfeat\n",
            "    Found existing installation: cyvlfeat 0.5.1\n",
            "    Uninstalling cyvlfeat-0.5.1:\n",
            "      Successfully uninstalled cyvlfeat-0.5.1\n",
            "  Running setup.py develop for cyvlfeat\n",
            "Successfully installed cyvlfeat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3985gb9aOypG"
      },
      "source": [
        "###  0-2: Connect to your Google Drive.\n",
        "\n",
        "It is required for loading the data.\n",
        "\n",
        "Enter your authorization code to access your drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKffRxrvDSJX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "780ceeb9-9bb1-4aba-a368-3d9002b67b51"
      },
      "source": [
        "# mount drive https://datascience.stackexchange.com/questions/29480/uploading-images-folder-from-my-system-into-google-colab\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bypm5tteROL"
      },
      "source": [
        "### 0-3: Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W88TOaCsxpfw"
      },
      "source": [
        "# Import libraries\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import cyvlfeat\n",
        "import time\n",
        "import scipy\n",
        "import multiprocessing"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xv7wrsXBO-w"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTq8GkOJBN4b"
      },
      "source": [
        "def euclidean_dist(x, y):\n",
        "    \"\"\"\n",
        "    :param x: [m, d]\n",
        "    :param y: [n, d]\n",
        "    :return:[m, n]\n",
        "    \"\"\"\n",
        "    m, n = x.shape[0], y.shape[0]    \n",
        "    eps = 1e-6 \n",
        "\n",
        "    xx = np.tile(np.power(x, 2).sum(axis=1), (n,1)) #[n, m]\n",
        "    xx = np.transpose(xx) # [m, n]\n",
        "    yy = np.tile(np.power(y, 2).sum(axis=1), (m,1)) #[m, n]\n",
        "    xy = np.matmul(x, np.transpose(y)) # [m, n]\n",
        "    dist = np.sqrt(xx + yy - 2*xy + eps)\n",
        "\n",
        "    return dist\n",
        "\n",
        "def read_img(image_path):\n",
        "    img = Image.open(image_path).convert('L')\n",
        "    img = img.resize((480, 480))\n",
        "    return np.float32(np.array(img)/255.)\n",
        "\n",
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        data = f.read()\n",
        "    return data.split()\n",
        "    \n",
        "def dataset_setup(data_dir):\n",
        "    train_file_list = []\n",
        "    val_file_list = []\n",
        "\n",
        "    for class_name in ['aeroplane','background','car','horse','motorbike','person']:\n",
        "        train_txt_path = os.path.join(data_dir, class_name+'_train.txt')\n",
        "        train_file_list.append(np.array(read_txt(train_txt_path)))\n",
        "        val_txt_path = os.path.join(data_dir, class_name+'_val.txt')\n",
        "        val_file_list.append(np.array(read_txt(val_txt_path)))\n",
        "\n",
        "    train_file_list = np.unique(np.concatenate(train_file_list))\n",
        "    val_file_list = np.unique(np.concatenate(val_file_list))\n",
        "\n",
        "    f = open(os.path.join(data_dir, \"train.txt\"), 'w')\n",
        "    for i in range(train_file_list.shape[0]):\n",
        "        data = \"%s\\n\" % train_file_list[i]\n",
        "        f.write(data)\n",
        "    f.close()\n",
        "\n",
        "    f = open(os.path.join(data_dir, \"val.txt\"), 'w')\n",
        "    for i in range(val_file_list.shape[0]):\n",
        "        data = \"%s\\n\" % val_file_list[i]\n",
        "        f.write(data)\n",
        "    f.close()\n",
        "\n",
        "def load_train_data(data_dir):\n",
        "    dataset_setup(data_dir)\n",
        "    num_proc = 12 # num_process\n",
        "\n",
        "    txt_path = os.path.join(data_dir, 'train.txt')\n",
        "    file_list = read_txt(txt_path)\n",
        "    image_paths = [os.path.join(data_dir+'/images', file_name+'.jpg') for file_name in file_list]\n",
        "    with multiprocessing.Pool(num_proc) as pool:\n",
        "      imgs = pool.map(read_img, image_paths)\n",
        "      imgs = np.array(imgs)\n",
        "      idxs = np.array(file_list)\n",
        "\n",
        "    return imgs, idxs\n",
        "\n",
        "def load_val_data(data_dir):\n",
        "    dataset_setup(data_dir)\n",
        "    num_proc = 12 # num_process\n",
        "\n",
        "    txt_path = os.path.join(data_dir, 'val.txt')\n",
        "    file_list = read_txt(txt_path)\n",
        "    image_paths = [os.path.join(data_dir+'/images', file_name+'.jpg') for file_name in file_list]\n",
        "    with multiprocessing.Pool(num_proc) as pool:\n",
        "      imgs = pool.map(read_img, image_paths)\n",
        "      imgs = np.array(imgs)\n",
        "      idxs = np.array(file_list)\n",
        "    \n",
        "    return imgs, idxs\n",
        "\n",
        "def get_labels(idxs, target_idxs):\n",
        "    \"\"\"\n",
        "    Get the labels from file index(name).\n",
        "\n",
        "    :param idxs(numpy.array): file index(name). shape:[num_images, ]\n",
        "    :param target_idxs(numpy.array): target index(name). shape:[num_target,]\n",
        "    :return(numpy.array): Target label(Binary label consisting of True and False). shape:[num_images,]\n",
        "    \"\"\"\n",
        "    return np.isin(idxs, target_idxs)\n",
        "\n",
        "def load_train_idxs(data_dir):\n",
        "    txt_path = os.path.join(data_dir, 'train.txt')\n",
        "    train_idxs = np.array(read_txt(txt_path))\n",
        "    return train_idxs\n",
        "\n",
        "def load_val_idxs(data_dir):\n",
        "    txt_path = os.path.join(data_dir, 'val.txt')\n",
        "    val_idxs = np.array(read_txt(txt_path))\n",
        "    return val_idxs"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c5F-N9wfzZW"
      },
      "source": [
        "## Step 1: Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYFEIkL24nJ5"
      },
      "source": [
        "''' \n",
        "Set your data path for loading images & labels.\n",
        "Example) CS_DATA_DIR = '/gdrive/My Drive/data'\n",
        "'''\n",
        "%env CS_DATA_DIR=/gdrive/My Drive/data\n",
        "!mkdir -p \"$CS_DATA_DIR\"\n",
        "!cd \"$CS_DATA_DIR\" && wget http://www.di.ens.fr/willow/events/cvml2013/materials/practicals/category-level/practical-category-recognition-2013a-data-only.tar.gz && tar -zxf practical-category-recognition-2013a-data-only.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GW7H_2iPxzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef4134b-88b5-4523-f21b-bba297f7f70a"
      },
      "source": [
        "category = ['aeroplane', 'car', 'horse', 'motorbike', 'person'] # DON'T MODIFY THIS. \n",
        "%env CS_DATA_DIR=/gdrive/My Drive/data\n",
        "data_dir = os.path.join(os.environ[\"CS_DATA_DIR\"], \"practical-category-recognition-2013a\", \"data\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "env: CS_DATA_DIR=/gdrive/My Drive/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX17mbhpXrNd"
      },
      "source": [
        "## Step 2: Bag of Visual Words (BoVW) Construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QuLZmSxX2l5"
      },
      "source": [
        "### 2-1. (**Problem 1**): SIFT descriptor extraction & Save the descriptors (10pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EWqpgbOV6yE"
      },
      "source": [
        "def SIFT_extraction(imgs):\n",
        "    \"\"\"\n",
        "    Extract Local SIFT descriptors from images using cyvlfeat.sift.sift().\n",
        "    Refer to https://github.com/menpo/cyvlfeat\n",
        "    You should set the parameters of cyvlfeat.sift.sift() as bellow.\n",
        "    1.compute_descriptor = True  2.float_descriptors = True\n",
        "\n",
        "    :param train_imgs(numpy.array): Gray-scale images in Numpy array format. shape:[num_images, width_size, height_size]\n",
        "    :return(numpy.array): SIFT descriptors. shape:[num_images, ], ndarray with object(descripotrs)\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE  \n",
        "    sift_feature = [(cyvlfeat.sift.sift(image=imgs[i], compute_descriptor=True, float_descriptors=True))[1] for i in range(imgs.shape[0])]\n",
        "    result = np.array(sift_feature, dtype=object)\n",
        "    del sift_feature\n",
        "    return result  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmVgUvbVYS2x"
      },
      "source": [
        "### 2-2. (**Problem 2**): Codebook(Bag of Visual Words) construction (10pt)\n",
        "In this step, you will construct the codebook using K-means clustering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLFB9eaw95zo"
      },
      "source": [
        "def get_codebook(des , k):\n",
        "  \"\"\"\n",
        "  Construct the codebook with visual codewords using k-means clustering.\n",
        "  In this step, you should use cyvlfeat.kmeans.kmeans().\n",
        "  Refer to https://github.com/menpo/cyvlfeat\n",
        "\n",
        "  :param des(numpy.array): Descriptors.  shape:[num_images, num_des_of_each_img, 128]\n",
        "  :param k(int): Number of visual words.\n",
        "  :return(numpy.array): Bag of visual words shape:[k, 128]\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  AllSIFTfeatures = np.concatenate(des, axis=0) \n",
        "  result = cyvlfeat.kmeans.kmeans(data=AllSIFTfeatures, num_centers=k)  \n",
        "  del AllSIFTfeatures\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH92-UOaYiM2"
      },
      "source": [
        "### 2-3. (**Problem 3**): Encode images to histogram feature based on codewords (10pt)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPQErulqCEKv"
      },
      "source": [
        "def extract_features(des, codebook):\n",
        "  \"\"\"\n",
        "  Construct the Bag-of-visual-Words histogram features for images using the codebook.\n",
        "  HINT: Refer to helper functions.\n",
        "\n",
        "  :param des(numpy.array): Descriptors.  shape:[num_images, num_des_of_each_img, 128]\n",
        "  :param codebook(numpy.array): Bag of visual words. shape:[k, 128]\n",
        "  :return(numpy.array): Bag of visual words shape:[num_images, k]\n",
        "\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  '''\n",
        "    (m=num_des_of_each_img, n=k)\n",
        "    dist = [num_des_of_each_img, k]\n",
        "    reference: https://kr.mathworks.com/help/vision/ug/image-classification-with-bag-of-visual-words.html\n",
        "  '''\n",
        "  histogram_features = [((np.apply_along_axis(lambda x: np.where(x==np.min(x), 1, 0), 1, euclidean_dist(des[i], codebook))).sum(axis=0)) for i in range(des.shape[0])]\n",
        "  result = np.array(histogram_features)\n",
        "  del histogram_features\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJwCm_b3YwYe"
      },
      "source": [
        "## Step 3. (**Problem 4**): Train the classifiers (10pt)\n",
        "Train a classifier using the sklearn library (SVC) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9gOjAvXXGJy"
      },
      "source": [
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkFInH3bDJPV"
      },
      "source": [
        "def train_classifier(features, labels, svm_params):\n",
        "  \"\"\"\n",
        "  Train the SVM classifier using sklearn.svm.svc()\n",
        "  Refer to https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "  :param features(numpy.array): Historgram representation. shape:[num_images, dim_feature]\n",
        "  :param labels(numpy.array): Target label(binary). shape:[num_images,]\n",
        "  :return(sklearn.svm.SVC): Trained classifier\n",
        "  \"\"\"\n",
        "  # Your code here \n",
        "  clf = SVC(**svm_params)\n",
        "  clf.fit(features, labels)\n",
        "  return clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNvZlykjWfyn"
      },
      "source": [
        "def Trainer(feat_params, svm_params):\n",
        "    \n",
        "    \"\"\"\n",
        "    Train the SVM classifier.\n",
        "\n",
        "    :param feat_params(dict): parameters for feature extraction.\n",
        "        ['extractor'](function pointer): function for extrat local descriptoers. (e.g. SIFT_extraction, DenseSIFT_extraction, etc)\n",
        "        ['num_codewords'](int):\n",
        "        ['result_dir'](str): Diretory to save codebooks & results.\n",
        "        \n",
        "    :param svm_params(dict): parameters for classifier training.\n",
        "        ['C'](float): Regularization parameter.\n",
        "        ['kernel'](str): Specifies the kernel type to be used in the algorithm.\n",
        "   \n",
        "    :return(sklearn.svm.SVC): trained classifier\n",
        "    \"\"\"\n",
        "    \n",
        "    extractor = feat_params['extractor']\n",
        "    k = feat_params['num_codewords']\n",
        "    result_dir = feat_params['result_dir']\n",
        "    \n",
        "    if not os.path.isdir(result_dir):\n",
        "        os.mkdir(result_dir)\n",
        "    \n",
        "    print(\"Load the training data...\")\n",
        "    start_time = time.time()\n",
        "    train_imgs, train_idxs = load_train_data(data_dir)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "    \n",
        "    print(\"Extract the local descriptors...\")\n",
        "    start_time = time.time()\n",
        "    train_des = extractor(train_imgs)\n",
        "    np.save(os.path.join(result_dir, 'train_des.npy'), train_des)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "    \n",
        "    del train_imgs\n",
        "    \n",
        "    print(\"Construct the bag of visual words...\")\n",
        "    start_time = time.time()\n",
        "    codebook = get_codebook(train_des, k)\n",
        "    np.save(os.path.join(result_dir, 'codebook.npy'), codebook)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "\n",
        "    print(\"Extract the image features...\")\n",
        "    start_time = time.time()    \n",
        "    train_features = extract_features(train_des, codebook)\n",
        "    np.save(os.path.join(result_dir, 'train_features.npy'), train_features)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "\n",
        "    del train_des, codebook\n",
        "    \n",
        "    print('Train the classifiers...')\n",
        "    accuracy = 0\n",
        "    models = {}\n",
        "    \n",
        "    for class_name in category:\n",
        "        target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_train.txt'.format(class_name)))])\n",
        "        target_labels = get_labels(train_idxs, target_idxs)\n",
        "        \n",
        "        models[class_name] = train_classifier(train_features, target_labels, svm_params)\n",
        "        train_accuracy = models[class_name].score(train_features, target_labels) \n",
        "        print('{} Classifier train accuracy:  {:.4f}'.format(class_name ,train_accuracy))\n",
        "        accuracy += train_accuracy\n",
        "    \n",
        "    print('Average train accuracy: {:.4f}'.format(accuracy/len(category)))\n",
        "    del train_features, target_labels, target_idxs\n",
        "\n",
        "    return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkM12brUWjLs"
      },
      "source": [
        "feat_params = {'extractor': SIFT_extraction, 'num_codewords':1024, 'result_dir':os.path.join(data_dir,'sift_1024')}\n",
        "svm_params = {'C': 1, 'kernel': 'linear'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0ULB-kc5jpk"
      },
      "source": [
        "- Below code will take about 30~70 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v_QngFiWlRZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ce5d2bf-5395-42fd-bfe4-84d7327b023b"
      },
      "source": [
        "models = Trainer(feat_params, svm_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load the training data...\n",
            "34.3899 seconds\n",
            "Extract the local descriptors...\n",
            "425.9539 seconds\n",
            "Construct the bag of visual words...\n",
            "4141.7708 seconds\n",
            "Extract the image features...\n",
            "101.7935 seconds\n",
            "Train the classifiers...\n",
            "aeroplane Classifier train accuracy:  1.0000\n",
            "car Classifier train accuracy:  1.0000\n",
            "horse Classifier train accuracy:  1.0000\n",
            "motorbike Classifier train accuracy:  1.0000\n",
            "person Classifier train accuracy:  0.9426\n",
            "Average train accuracy: 0.9885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLnPCHFOalSk"
      },
      "source": [
        "## Step 4: Test the classifier on validation set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EN0ZUiXWoI3"
      },
      "source": [
        "def Test(feat_params, models):\n",
        "    \"\"\"\n",
        "    Test the SVM classifier.\n",
        "\n",
        "    :param feat_params(dict): parameters for feature extraction.\n",
        "        ['extractor'](function pointer): function for extrat local descriptoers. (e.g. SIFT_extraction, DenseSIFT_extraction, etc)\n",
        "        ['num_codewords'](int):\n",
        "        ['result_dir'](str): Diretory to load codebooks & save results.\n",
        "        \n",
        "    :param models(dict): dict of classifiers(sklearn.svm.SVC)\n",
        "    \"\"\"\n",
        "    \n",
        "    extractor = feat_params['extractor']\n",
        "    k = feat_params['num_codewords']\n",
        "    result_dir = feat_params['result_dir']\n",
        "    \n",
        "    print(\"Load the validation data...\")\n",
        "    start_time = time.time()\n",
        "    val_imgs, val_idxs = load_val_data(data_dir)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "    \n",
        "    print(\"Extract the local descriptors...\")\n",
        "    start_time = time.time()\n",
        "    val_des = extractor(val_imgs)\n",
        "    np.save(os.path.join(result_dir, 'val_des.npy'), val_des)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "    \n",
        "    \n",
        "    del val_imgs\n",
        "    codebook = np.load(os.path.join(result_dir, 'codebook.npy'))\n",
        "    \n",
        "    print(\"Extract the image features...\")\n",
        "    start_time = time.time()    \n",
        "    val_features = extract_features(val_des, codebook)\n",
        "    np.save(os.path.join(result_dir, 'val_features.npy'), val_features)\n",
        "    print(\"{:.4f} seconds\".format(time.time()-start_time))\n",
        "\n",
        "    del val_des, codebook\n",
        "\n",
        "    print('Test the classifiers...')\n",
        "    accuracy = 0\n",
        "    for class_name in category:\n",
        "        target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_val.txt'.format(class_name)))])\n",
        "        target_labels = get_labels(val_idxs, target_idxs)\n",
        "        \n",
        "        val_accuracy = models[class_name].score(val_features, target_labels)\n",
        "        print('{} Classifier validation accuracy:  {:.4f}'.format(class_name ,val_accuracy))\n",
        "        accuracy += val_accuracy\n",
        "    \n",
        "    del val_features, target_idxs, target_labels\n",
        "\n",
        "    print('Average validation accuracy: {:.4f}'.format(accuracy/len(category)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z80KKyn7Ytfu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "865c3ba2-37c6-40fc-8eb4-9e2d6a9a2963"
      },
      "source": [
        "Test(feat_params ,models)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load the validation data...\n",
            "100.1162 seconds\n",
            "Extract the local descriptors...\n",
            "402.4537 seconds\n",
            "Extract the image features...\n",
            "104.6940 seconds\n",
            "Test the classifiers...\n",
            "aeroplane Classifier validation accuracy:  0.9406\n",
            "car Classifier validation accuracy:  0.7425\n",
            "horse Classifier validation accuracy:  0.9002\n",
            "motorbike Classifier validation accuracy:  0.9159\n",
            "person Classifier validation accuracy:  0.5829\n",
            "Average validation accuracy: 0.8164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3X0BFLm756K"
      },
      "source": [
        "## **Problem 5**: Implement Dense SIFT (10pt)\n",
        "Modify the feature extractor using the dense SIFT and evaluate the performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaY4kqQE8PXK"
      },
      "source": [
        "def DenseSIFT_extraction(imgs):\n",
        "  \"\"\"\n",
        "  Extract Dense SIFT descriptors from images using cyvlfeat.sift.dsift().\n",
        "  Refer to https://github.com/menpo/cyvlfeat\n",
        "  You should set the parameters of cyvlfeat.sift.dsift() as bellow.\n",
        "    1.step = 12  2.float_descriptors = True\n",
        "\n",
        "  :param train_imgs(numpy.array): Gray-scale images in Numpy array format. shape:[num_images, width_size, height_size]\n",
        "  :return(numpy.array): Dense SIFT descriptors. shape:[num_images, num_des_of_each_img, 128]\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE\n",
        "  dsift_feature = [(cyvlfeat.sift.dsift(image=imgs[i], step=12, float_descriptors=True))[1] for i in range(imgs.shape[0])] \n",
        "  result = np.zeros((imgs.shape[0], dsift_feature[0].shape[0], dsift_feature[0].shape[1])) \n",
        "  for i in range(imgs.shape[0]): \n",
        "    result[i] = np.array(dsift_feature[i])\n",
        "  del dsift_feature\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyhyW4yYEFxz"
      },
      "source": [
        "feat_params = {'extractor': DenseSIFT_extraction, 'num_codewords':1024, 'result_dir':os.path.join(data_dir,'dsift_1024')}\n",
        "svm_params = {'C': 1, 'kernel': 'linear'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPYn8ubgEuq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d16ba9-07c5-4d25-d00f-56bcc951f30f"
      },
      "source": [
        "models = Trainer(feat_params, svm_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load the training data...\n",
            "35.7257 seconds\n",
            "Extract the local descriptors...\n",
            "618.2104 seconds\n",
            "Construct the bag of visual words...\n",
            "8225.8245 seconds\n",
            "Extract the image features...\n",
            "203.8099 seconds\n",
            "Train the classifiers...\n",
            "aeroplane Classifier train accuracy:  1.0000\n",
            "car Classifier train accuracy:  1.0000\n",
            "horse Classifier train accuracy:  1.0000\n",
            "motorbike Classifier train accuracy:  1.0000\n",
            "person Classifier train accuracy:  0.9814\n",
            "Average train accuracy: 0.9963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b3X3gYHErl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc0ccda-4376-49a3-dcb4-7ec982bce059"
      },
      "source": [
        "Test(feat_params ,models)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load the validation data...\n",
            "35.7720 seconds\n",
            "Extract the local descriptors...\n",
            "570.1793 seconds\n",
            "Extract the image features...\n",
            "207.0575 seconds\n",
            "Test the classifiers...\n",
            "aeroplane Classifier validation accuracy:  0.9475\n",
            "car Classifier validation accuracy:  0.7882\n",
            "horse Classifier validation accuracy:  0.9155\n",
            "motorbike Classifier validation accuracy:  0.9074\n",
            "person Classifier validation accuracy:  0.6023\n",
            "Average validation accuracy: 0.8322\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWyoU1yG7qhf"
      },
      "source": [
        "## **Problem 6**: Implement the Spatial Pyramid (10pt)\n",
        "Modify the feature extractor using the spatial pyramid matching and evaluate the performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJJpyKo98QQp"
      },
      "source": [
        "def SpatialPyramid(des, codebook):\n",
        "  \"\"\"\n",
        "  Extract image representation with Spatial Pyramid Matching using your DenseSIFT descripotrs & codebook.\n",
        "\n",
        "  :param des(numpy.array): DenseSIFT Descriptors.  shape:[num_images, num_des_of_each_img, 128]\n",
        "  :param codebook(numpy.array): Bag of visual words. shape:[k, 128]\n",
        "\n",
        "  :return(numpy.array): Image feature using SpatialPyramid [num_images, features_dim]\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE \n",
        "      # https://darkpgmr.tistory.com/125 \n",
        "      # https://www.youtube.com/watch?v=6MwuK2wHlOg \n",
        "  '''\n",
        "  Therefore, \"all implementations which includes the advantage of Spatial Pyramid will be fine.\" \n",
        "  Simply concatenating the features would be one of them.\n",
        "  ''' \n",
        "  # Level0, Level1; (num_images,1600,128),(num_images,400,128) \n",
        "  idx_cut_level1 = int(des.shape[1]/4) # 1600/4 = 400\n",
        "  h0 = extract_features(des, codebook) # (num_images, k)\n",
        "  h1_0 = extract_features(des[:,:idx_cut_level1,:], codebook) # (num_images, k)\n",
        "  h1_1 = extract_features(des[:,idx_cut_level1:idx_cut_level1*2,:], codebook) # (num_images, k)\n",
        "  h1_2 = extract_features(des[:,idx_cut_level1*2:idx_cut_level1*3,:], codebook) # (num_images, k)\n",
        "  h1_3 = extract_features(des[:,idx_cut_level1*3:idx_cut_level1*4,:], codebook) # (num_images, k)\n",
        "  result = np.concatenate((h0, h1_0, h1_1, h1_2, h1_3), axis=1) # (num_images, 5k) \n",
        "  del h0, h1_0, h1_1, h1_2, h1_3\n",
        "\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAVKLXXyx2NE"
      },
      "source": [
        "def SP_Trainer(des_path, codebook_path, result_dir, svm_params):\n",
        "    \n",
        "    \"\"\"\n",
        "    Train the SVM classifier using SpatialPyramid representations.\n",
        "\n",
        "    :param des_path(str): path for loading training dataset DenseSIFT descriptors.\n",
        "    :param codebook(str): path for loading codebook for DenseSIFT descriptors.\n",
        "    :param result_dir(str): diretory to save features.\n",
        "        \n",
        "    :param svm_params(dict): parameters for classifier training.\n",
        "        ['C'](float): Regularization parameter.\n",
        "        ['kernel'](str): Specifies the kernel type to be used in the algorithm.\n",
        "   \n",
        "    :return(sklearn.svm.SVC): trained classifier\n",
        "    \"\"\"\n",
        "    train_idxs = load_train_idxs(data_dir)\n",
        "    train_des = np.load(des_path)\n",
        "    codebook = np.load(codebook_path)\n",
        "    train_features = SpatialPyramid(train_des, codebook)\n",
        "    np.save(os.path.join(result_dir, 'train_sp_features.npy'), train_features)\n",
        "\n",
        "    del train_des, codebook\n",
        "    \n",
        "    print('Train the classifiers...')\n",
        "    accuracy = 0\n",
        "    models = {}\n",
        "    \n",
        "    for class_name in category:\n",
        "        target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_train.txt'.format(class_name)))])\n",
        "        target_labels = get_labels(train_idxs, target_idxs)\n",
        "        \n",
        "        models[class_name] = train_classifier(train_features, target_labels, svm_params)\n",
        "        train_accuracy = models[class_name].score(train_features, target_labels) \n",
        "        print('{} Classifier train accuracy:  {:.4f}'.format(class_name ,train_accuracy))\n",
        "        accuracy += train_accuracy\n",
        "    \n",
        "    print('Average train accuracy: {:.4f}'.format(accuracy/len(category)))\n",
        "    del train_features, target_labels, target_idxs\n",
        "\n",
        "    return models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q--UT0fyEyc"
      },
      "source": [
        "def SP_Test(des_path, codebook_path, result_dir, models):\n",
        "    \"\"\"\n",
        "    Test the SVM classifier.\n",
        "\n",
        "    :param des_path(str): path for loading validation dataset DenseSIFT descriptors.\n",
        "    :param codebook(str): path for loading codebook for DenseSIFT descriptors.\n",
        "    :param result_dir(str): diretory to save features.      \n",
        "    :param models(dict): dict of classifiers(sklearn.svm.SVC)\n",
        "\n",
        "    \"\"\" \n",
        "    val_idxs = load_val_idxs(data_dir)\n",
        "    val_des = np.load(des_path)\n",
        "    codebook = np.load(codebook_path)\n",
        "    val_features = SpatialPyramid(val_des, codebook)\n",
        "    np.save(os.path.join(result_dir, 'val_sp_features.npy'), val_features)\n",
        "\n",
        "\n",
        "    del val_des, codebook\n",
        "\n",
        "    print('Test the classifiers...')\n",
        "    accuracy = 0\n",
        "    for class_name in category:\n",
        "        target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_val.txt'.format(class_name)))])\n",
        "        target_labels = get_labels(val_idxs, target_idxs)\n",
        "        \n",
        "        val_accuracy = models[class_name].score(val_features, target_labels)\n",
        "        print('{} Classifier validation accuracy:  {:.4f}'.format(class_name ,val_accuracy))\n",
        "        accuracy += val_accuracy\n",
        "\n",
        "    del val_features, target_idxs, target_labels\n",
        "\n",
        "    print('Average validation accuracy: {:.4f}'.format(accuracy/len(category)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS7Svvy2zTv_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9235372f-8f44-48a7-9aa9-003b16aeaeec"
      },
      "source": [
        "#YOUR CODE HERE for training & testing with Spatial Pyramid \n",
        "  # Train\n",
        "result_dir = os.path.join(data_dir,'dsift_1024') \n",
        "des_path = result_dir+'/train_des.npy'\n",
        "codebook_path = result_dir+'/codebook.npy'\n",
        "svm_params = {'C': 1, 'kernel': 'linear'}   \n",
        "models = SP_Trainer(des_path, codebook_path, result_dir, svm_params)\n",
        "\n",
        "  # Test\n",
        "print('\\n')\n",
        "des_path = result_dir+'/val_des.npy'\n",
        "SP_Test(des_path, codebook_path, result_dir, models)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train the classifiers...\n",
            "aeroplane Classifier train accuracy:  1.0000\n",
            "car Classifier train accuracy:  1.0000\n",
            "horse Classifier train accuracy:  1.0000\n",
            "motorbike Classifier train accuracy:  1.0000\n",
            "person Classifier train accuracy:  1.0000\n",
            "Average train accuracy: 1.0000\n",
            "\n",
            "\n",
            "Test the classifiers...\n",
            "aeroplane Classifier validation accuracy:  0.9604\n",
            "car Classifier validation accuracy:  0.8294\n",
            "horse Classifier validation accuracy:  0.9458\n",
            "motorbike Classifier validation accuracy:  0.9430\n",
            "person Classifier validation accuracy:  0.6342\n",
            "Average validation accuracy: 0.8626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "401jsdB_8CA1"
      },
      "source": [
        "## **Problem 7**: Improve classification using non-linear SVM (10pt)\n",
        "Modify the classifier using the non-linear SVM and evaluate the performance. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg162rmJ8Q8S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4fe984-b996-44ce-ddeb-a40be77e641c"
      },
      "source": [
        "# YOUR CODE HERE to improve classification using non-linear SVM\n",
        "# YOUR CODE should include training & testing with non-linear SVM.\n",
        "svm_params = {'C': 1, 'kernel': 'rbf'}\n",
        "\n",
        "# Train\n",
        "print('Train the classifiers...')\n",
        "train_imgs, train_idxs = load_train_data(data_dir)\n",
        "train_features = np.load(os.path.join(data_dir,'dsift_1024')+'/train_sp_features.npy')\n",
        "\n",
        "accuracy = 0\n",
        "models = {}\n",
        "\n",
        "for class_name in category:\n",
        "    target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_train.txt'.format(class_name)))])\n",
        "    target_labels = get_labels(train_idxs, target_idxs)\n",
        "    \n",
        "    models[class_name] = train_classifier(train_features, target_labels, svm_params)\n",
        "    train_accuracy = models[class_name].score(train_features, target_labels) \n",
        "    print('{} Classifier train accuracy:  {:.4f}'.format(class_name ,train_accuracy))\n",
        "    accuracy += train_accuracy\n",
        "\n",
        "print('Average train accuracy: {:.4f}'.format(accuracy/len(category)))\n",
        "del train_features, target_labels, target_idxs \n",
        "\n",
        "# Test\n",
        "print('\\nTest the classifiers...')\n",
        "val_imgs, val_idxs = load_val_data(data_dir) \n",
        "val_features = np.load(os.path.join(data_dir,'dsift_1024')+'/val_sp_features.npy')\n",
        "\n",
        "accuracy = 0\n",
        "\n",
        "for class_name in category:\n",
        "    target_idxs = np.array([read_txt(os.path.join(data_dir, '{}_val.txt'.format(class_name)))])\n",
        "    target_labels = get_labels(val_idxs, target_idxs)\n",
        "    \n",
        "    val_accuracy = models[class_name].score(val_features, target_labels)\n",
        "    print('{} Classifier validation accuracy:  {:.4f}'.format(class_name ,val_accuracy))\n",
        "    accuracy += val_accuracy\n",
        "\n",
        "print('Average validation accuracy: {:.4f}'.format(accuracy/len(category)))\n",
        "del val_features, target_idxs, target_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train the classifiers...\n",
            "aeroplane Classifier train accuracy:  0.9786\n",
            "car Classifier train accuracy:  0.8997\n",
            "horse Classifier train accuracy:  0.9498\n",
            "motorbike Classifier train accuracy:  0.9547\n",
            "person Classifier train accuracy:  0.8936\n",
            "Average train accuracy: 0.9353\n",
            "\n",
            "Test the classifiers...\n",
            "aeroplane Classifier validation accuracy:  0.9568\n",
            "car Classifier validation accuracy:  0.8715\n",
            "horse Classifier validation accuracy:  0.9406\n",
            "motorbike Classifier validation accuracy:  0.9495\n",
            "person Classifier validation accuracy:  0.6952\n",
            "Average validation accuracy: 0.8827\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b8Z-UnocePF"
      },
      "source": [
        "# <font color=\"blue\"> Discussion and Analysis </font>\n",
        "## Discussion Guidelines\n",
        "- You should write discussion about **Problem 5 ~ Problem 7**.\n",
        "- Simply reporting the results (e.g. classification accuracy) is not considered as a discussion.\n",
        "- For each problem's discussion, you should explain and compare how each method improves the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCn1KAvNc38r"
      },
      "source": [
        "Please write discussions on the results above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blxNfZ8_-8Ui"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "The results (SIFT + Linear SVM) were below:\n",
        "\n",
        "**Train Accuracy**\n",
        "\n",
        "|Category|Accuracy|\n",
        "|:--------:|:-------:|\n",
        "|aeroplane Classifier|1.0000|\n",
        "|car Classifier|1.0000|\n",
        "|horse Classifier|1.0000|\n",
        "|motorbike Classifier|1.0000|\n",
        "|person Classifier|0.9426|\n",
        "|**Average**|**0.9885**|\n",
        "\n",
        "**Test Accuracy**\n",
        "\n",
        "|Category|Accuracy|\n",
        "|:--------:|:-------:|\n",
        "|aeroplane Classifier|0.9406|\n",
        "|car Classifier|0.7425|\n",
        "|horse Classifier|0.9002|\n",
        "|motorbike Classifier|0.9159|\n",
        "|person Classifier|0.5829|\n",
        "|**Average**|**0.8164**| "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8vjpK05vjVF"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "**Problem 5**: Dense SIFT + Linear SVM\n",
        "----------------------------------------------------\n",
        "reference : https://www.quora.com/Why-does-Dense-SIFT-perform-better-or-even-comparable-than-SIFT\n",
        "\n",
        "In general, the SIFT algorithm calculates a descriptor at a specific location of the extracted key-point. However, Dense SIFT assumes feature points that are evenly concentrated at regular intervals in the entire area of an image and calculates a descriptor from it. As a result, Dense SIFT generally performs better than SIFT because it searches as many descriptors as possible in an image.\n",
        "\n",
        "**Train Accuracy**\n",
        "\n",
        "|Category|Accuracy|\n",
        "|:--------:|:-------:|\n",
        "|aeroplane Classifier|1.0000|\n",
        "|car Classifier|1.0000|\n",
        "|horse Classifier|1.0000|\n",
        "|motorbike Classifier|1.0000|\n",
        "|person Classifier|0.9814|\n",
        "|**Average**|**0.9963**|\n",
        "\n",
        "**Test Accuracy**\n",
        "\n",
        "|Category|Accuracy|\n",
        "|:--------:|:-------:|\n",
        "|aeroplane Classifier|0.9475|\n",
        "|car Classifier|0.7882|\n",
        "|horse Classifier|0.9155|\n",
        "|motorbike Classifier|0.9074|\n",
        "|person Classifier|0.6023|\n",
        "|**Average**|**0.8322**| \n",
        "\n",
        "It was confirmed that the  test accuracy was improved from 0.8164 (SIFT + Linear SVM) to 0.8322 (Dense SIFT + Linear SVM)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9RkZJKgvpaR"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "**Problem 6**: Dense SIFT + Spatial Pyramid + Linear SVM \n",
        "----------------------------------------------------\n",
        "The Bag of Visual Words (BoVW) method has a problem of losing the geometrical positional relationship between features because it basically expresses an image as a histogram of features obtained from the entire image area. In other words, even if the codeword is the same, if the position of the codeword is different, the histrogram appears differently. To make up for this shortcoming, Spatial Pyramid was used.\n",
        "The Spatial Pyramid method divides an image into several steps, obtains a histogram for each segmented area, and compares them as a whole.\n",
        "\n",
        "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
        "\n",
        "temp = (cyvlfeat.sift.dsift(image=np.random.rand(480,480), step=12,float_descriptors=True)[0]) \n",
        "\n",
        "print(temp[0]) # [4.5, 4.5]\n",
        "\n",
        "print(temp[1]) # [4.5, 16.5]\n",
        "\n",
        "print(temp[2]) # [4.5, 28.5]\n",
        "\n",
        "-> **Row-major order**\n",
        "\n",
        "'''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
        "\n",
        "\n",
        "When the histogram generated at level 0 (original) of the input image is h0, and the histogram generated at level 1 (4x1 division) is h1_0, h1_1, h1_2, h1_3 (in order), concatenation is performed to make a single feature.  Then, the feature for one image is (h0, h1_0, h1_1, h1_2, h1_3) whose size is (1, 5*k) (assuming the size of the codebook is k). \n",
        "\n",
        "To sum up, Each Histrogram in the divided area can hold spatial information.\n",
        "\n",
        "The results (Dense SIFT + Spatial Pyramid + Linear SVM) were below: \n",
        "\n",
        "**Train Accuracy**\n",
        "\n",
        "|Category|Accuracy|\n",
        "|:--------:|:-------:|\n",
        "|aeroplane Classifier|1.0000|\n",
        "|car Classifier|1.0000|\n",
        "|horse Classifier|1.0000|\n",
        "|motorbike Classifier|1.0000|\n",
        "|person Classifier|1.0000|\n",
        "|**Average**|**1.0000**|\n",
        "\n",
        "**Test Accuracy**\n",
        "\n",
        "|Category|Accuracy|\n",
        "|:--------:|:-------:|\n",
        "|aeroplane Classifier|0.9604|\n",
        "|car Classifier|0.8294|\n",
        "|horse Classifier|0.9458|\n",
        "|motorbike Classifier|0.9430|\n",
        "|person Classifier|0.6342|\n",
        "|**Average**|**0.8626**|\n",
        "\n",
        "It was confirmed that the test accuracy was improved from 0.8322 (Dense SIFT + Linear SVM) to 0.8626 (Dense SIFT + Spatial Pyramid + Linear SVM).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frfFaOyU7v9U"
      },
      "source": [
        "--------------------------------------------------------------------------------\n",
        "**Problem 7**: Dense SIFT + Spatial Pyramid + Non-Linear SVM\n",
        "----------------------------------------------------\n",
        "SVM is a technique to find a hyperplane with maximized margin while classifying two categories well. However, it is difficult to classify actual data linearly.\n",
        "\n",
        "The basic idea of ​​Kernel-SVM is to map the data from the input space to a feature space that can be linearly separated. In other words, the kernel method (non-linear) maps the given data into a high-dimensional feature space. After being mapped in a high-dimensional space, there is a way to classify it into a linear shape that was not visible in the original dimension. \n",
        "\n",
        "There are many types of kernels such as Polynomial kernel, Sigmoid kernel, Gaussian RBF kernel, and I chose Gaussian RBF kernel among them. Although the parameters were used as defaults without empirically finding optimal parameter values ​​through hyperparamter tuning(GridSearch), the performance of Non-Linear SVM improved compared to Linear SVM.\n",
        "\n",
        "The results (Dense SIFT + Spatial Pyramid + Non-Linear SVM) were below: \n",
        "\n",
        "**Train Accuracy**\n",
        "\n",
        "|Category|Accuracy|\n",
        "|:--------:|:-------:|\n",
        "|aeroplane Classifier|0.9786|\n",
        "|car Classifier|0.8997|\n",
        "|horse Classifier|0.9498|\n",
        "|motorbike Classifier|0.9547|\n",
        "|person Classifier|0.8936|\n",
        "|**Average**|**0.9353**|\n",
        "\n",
        "**Test Accuracy**\n",
        "\n",
        "|Category|Accuracy|\n",
        "|:--------:|:-------:|\n",
        "|aeroplane Classifier|0.9568|\n",
        "|car Classifier|0.8715|\n",
        "|horse Classifier|0.9406|\n",
        "|motorbike Classifier|0.9495|\n",
        "|person Classifier|0.6952|\n",
        "|**Average**|**0.8827**| \n",
        "\n",
        "It was confirmed that the test accuracy was improved from 0.8626 (Dense SIFT + Spatial Pyramid + Linear SVM) to 0.8827 (Dense SIFT + Spatial Pyramid + Non-Linear SVM)."
      ]
    }
  ]
}